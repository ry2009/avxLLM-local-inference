{"prompt": "Question: 2 + 2 = ?\nOptions:\n(A) 3\n(B) 4\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": "  (A)", "correct": false, "details": {"choice": "A"}, "duration_s": 1.9802849200204946, "length_chars": 5}, {"completion": "  (A)", "correct": false, "details": {"choice": "A"}, "duration_s": 0.5279318259563297, "length_chars": 5}, {"completion": "  (A)", "correct": false, "details": {"choice": "A"}, "duration_s": 0.642495307023637, "length_chars": 5}, {"completion": "  (A)", "correct": false, "details": {"choice": "A"}, "duration_s": 5.633796070003882, "length_chars": 5}, {"completion": "  (A)", "correct": false, "details": {"choice": "A"}, "duration_s": 0.5730078620254062, "length_chars": 5}, {"completion": "  (A)", "correct": false, "details": {"choice": "A"}, "duration_s": 0.5585897320415825, "length_chars": 5}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 3, "avg_duration_s": 1.6526842861785553, "avg_chars_per_sec": 3.0253812187936555}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: What is the capital of Germany?\nOptions:\n(A) Berlin\n(B) Munich\n(C) Hamburg\nAnswer:", "answer": "A", "samples": [{"completion": "\n(B)\n(C)\n(D)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.2411325420252979, "length_chars": 12}, {"completion": "\n(B)\n(C)\n(A)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.496049554029014, "length_chars": 12}, {"completion": "\n(B)\n(C)\n(A)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.215103022987023, "length_chars": 12}, {"completion": "\n(B)\n(C)\n(D)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.2212521079927683, "length_chars": 12}, {"completion": "\n(B)\n(C)\n(A)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.315157959004864, "length_chars": 12}, {"completion": "\n(B)\n(C)\n(D)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.2902087639668025, "length_chars": 12}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.3333333333333333, "entropy": 0.6931471805579453, "avg_completion_length": 11, "avg_duration_s": 1.2964839916676283, "avg_chars_per_sec": 9.255802676410035}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: Which planet is known as the Red Planet?\nOptions:\n(A) Venus\n(B) Mars\n(C) Jupiter\nAnswer:", "answer": "B", "samples": [{"completion": "  (B)\n\n(A)\n\n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.2691972549655475, "length_chars": 13}, {"completion": "  (B)\n\n(A)\n\n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1515887320274487, "length_chars": 13}, {"completion": "  (B)\n\n(A)\n\n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1677242129808292, "length_chars": 13}, {"completion": "  (B)\n\n(A)\n\n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.2756164969759993, "length_chars": 13}, {"completion": "  (B)\n\n(A)\n\n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.2542906280141324, "length_chars": 13}, {"completion": "  (B)\n\n(A)\n\n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.148767160018906, "length_chars": 13}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 11, "avg_duration_s": 1.2111974141638104, "avg_chars_per_sec": 10.733180114139339}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: 9 * 7 = ?\nOptions:\n(A) 56\n(B) 63\n(C) 72\nAnswer:", "answer": "B", "samples": [{"completion": "  (A)\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.8876660999958403, "length_chars": 9}, {"completion": "  (A)\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.9063451500260271, "length_chars": 9}, {"completion": "  (A)\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.9740694389911368, "length_chars": 9}, {"completion": "  (A)\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.899885262013413, "length_chars": 9}, {"completion": "  (A)\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.8627634319709614, "length_chars": 9}, {"completion": "  (A)\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.8902603419846855, "length_chars": 9}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 7, "avg_duration_s": 0.9034982874970107, "avg_chars_per_sec": 9.961280640534449}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: Who wrote '1984'?\nOptions:\n(A) George Orwell\n(B) Aldous Huxley\n(C) Ray Bradbury\nAnswer:", "answer": "A", "samples": [{"completion": "\n(B)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 0.9288882089895196, "length_chars": 8}, {"completion": "\n(B)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 0.9155872350092977, "length_chars": 8}, {"completion": "\n(B)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 0.9156035549822263, "length_chars": 8}, {"completion": "\n(B)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 0.9820208479650319, "length_chars": 8}, {"completion": "\n(B)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 0.8979009349714033, "length_chars": 8}, {"completion": "\n(B)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 0.8969116939697415, "length_chars": 8}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 7, "avg_duration_s": 0.9228187459812034, "avg_chars_per_sec": 8.669091340893663}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: Water's chemical formula is?\nOptions:\n(A) H2O\n(B) CO2\n(C) O2\nAnswer:", "answer": "A", "samples": [{"completion": "  (A) H2O\n\nA:", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1545138810179196, "length_chars": 13}, {"completion": "  (A) H2O\n\nA:", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1623538649873808, "length_chars": 13}, {"completion": "  (A) H2O\n\nA:", "correct": true, "details": {"choice": "A"}, "duration_s": 1.3094417069805786, "length_chars": 13}, {"completion": "  (A) H2O\n\nA:", "correct": true, "details": {"choice": "A"}, "duration_s": 1.2167701839935035, "length_chars": 13}, {"completion": "  (A) H2O\n\nA:", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1741250570048578, "length_chars": 13}, {"completion": "  (A) H2O\n\nA:", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1838889310020022, "length_chars": 13}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 11, "avg_duration_s": 1.2001822708310403, "avg_chars_per_sec": 10.831688082676335}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: 15 divided by 5 equals?\nOptions:\n(A) 2\n(B) 3\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": "  2\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.696347292046994, "length_chars": 7}, {"completion": "  2\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.7095278050401248, "length_chars": 7}, {"completion": "  2\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.6863618480274454, "length_chars": 7}, {"completion": "  2\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.6952319649863057, "length_chars": 7}, {"completion": "  2\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.6986707139876671, "length_chars": 7}, {"completion": "  2\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 0.6904661979642697, "length_chars": 7}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 5, "avg_duration_s": 0.6961009703421345, "avg_chars_per_sec": 10.056012415209667}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: Largest mammal?\nOptions:\n(A) Elephant\n(B) Blue Whale\n(C) Giraffe\nAnswer:", "answer": "B", "samples": [{"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8957712789997458, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8834570379694924, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8864056480233558, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.9139222179655917, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8949107949738391, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8843919110368006, "length_chars": 9}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 7, "avg_duration_s": 0.8931431481614709, "avg_chars_per_sec": 10.076772148480833}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: Which language is primarily spoken in Brazil?\nOptions:\n(A) Spanish\n(B) Portuguese\n(C) French\nAnswer:", "answer": "B", "samples": [{"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.859260292025283, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8729561229702085, "length_chars": 9}, {"completion": "  (B)  \n(A)  \n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.150494590983726, "length_chars": 15}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8945946730091237, "length_chars": 9}, {"completion": "  (B)  \n(A)  \n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1429812610149384, "length_chars": 15}, {"completion": "  (B)  \n(A)  \n(", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1491093499935232, "length_chars": 15}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.3333333333333333, "entropy": 0.6931471805579453, "avg_completion_length": 10, "avg_duration_s": 1.0115660483328004, "avg_chars_per_sec": 11.862794347218005}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
{"prompt": "Question: What is 12 - 5?\nOptions:\n(A) 6\n(B) 7\n(C) 8\nAnswer:", "answer": "B", "samples": [{"completion": "  (B)", "correct": true, "details": {"choice": "B"}, "duration_s": 0.5006273349863477, "length_chars": 5}, {"completion": "  (B)", "correct": true, "details": {"choice": "B"}, "duration_s": 0.5139077409985475, "length_chars": 5}, {"completion": "  (B)", "correct": true, "details": {"choice": "B"}, "duration_s": 0.513267946022097, "length_chars": 5}, {"completion": "  (B)", "correct": true, "details": {"choice": "B"}, "duration_s": 0.5312925860052928, "length_chars": 5}, {"completion": "  (B)", "correct": true, "details": {"choice": "B"}, "duration_s": 0.5274187950417399, "length_chars": 5}, {"completion": "  (B)", "correct": true, "details": {"choice": "B"}, "duration_s": 0.5151761819724925, "length_chars": 5}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 3, "avg_duration_s": 0.5169484308377529, "avg_chars_per_sec": 9.672144650670731}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044343Z"}
