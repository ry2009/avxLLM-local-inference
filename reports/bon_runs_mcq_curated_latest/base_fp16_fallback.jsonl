{"prompt": "Question: 2 + 2 = ?\nOptions:\n(A) 3\n(B) 4\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": " 4 + 4 = 6\n\nI have tried it with", "correct": false, "details": {"choice": "I"}, "duration_s": 2.8040099929785356, "length_chars": 32}, {"completion": " 5 + 2 = ?\n\nA:\n\nI", "correct": false, "details": {"choice": "A"}, "duration_s": 1.133744059014134, "length_chars": 17}, {"completion": " (A) + (A) + (B) +", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1397510900278576, "length_chars": 18}, {"completion": " ?\n\nA:\n\nThe answer is no.", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1340410090051591, "length_chars": 25}, {"completion": " 3 + 4 = ?\n\nA:\n\nThe", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1319676599814557, "length_chars": 19}, {"completion": " 5\n\nA:\n\nAssuming, I've understood", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1156486779800616, "length_chars": 33}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 23, "avg_duration_s": 1.4098604148312006, "avg_chars_per_sec": 17.02296181063674}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: What is the capital of Germany?\nOptions:\n(A) Berlin\n(B) Munich\n(C) Hamburg\nAnswer:", "answer": "A", "samples": [{"completion": " Germany\nOptions:\n(A)\n(B)", "correct": false, "details": {"choice": "G"}, "duration_s": 1.1218566780444235, "length_chars": 25}, {"completion": " B\n\n(A) Berlin\n\n(C)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1618431969545782, "length_chars": 19}, {"completion": " Germany\n\nOption 1:\n(A) Berlin\n", "correct": false, "details": {"choice": "G"}, "duration_s": 1.2090304629527964, "length_chars": 31}, {"completion": " Berlin\n\nIn this case, the text in question could", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1573246640036814, "length_chars": 49}, {"completion": "\n(A) The capital of Germany is Berlin, which", "correct": true, "details": {"choice": "A"}, "duration_s": 1.10592107498087, "length_chars": 44}, {"completion": " B\n\nWe ask about the names of the cities and", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1083690510131419, "length_chars": 44}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 34.166666666666664, "avg_duration_s": 1.1440575213249151, "avg_chars_per_sec": 30.88422799966767}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: Which planet is known as the Red Planet?\nOptions:\n(A) Venus\n(B) Mars\n(C) Jupiter\nAnswer:", "answer": "B", "samples": [{"completion": " Jupiter\nAnswer: Jupiter\nChoose a planet, and answer", "correct": false, "details": {"choice": "J"}, "duration_s": 1.1373719619587064, "length_chars": 52}, {"completion": " Jupiter\nI know there are many planets like this. I", "correct": false, "details": {"choice": "J"}, "duration_s": 1.0909083139849827, "length_chars": 51}, {"completion": " (A)\n(B)\n(C)\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.4641175639699213, "length_chars": 13}, {"completion": " (C)\n1. Venus\nVenus is the", "correct": false, "details": {"choice": "C"}, "duration_s": 1.4341396589879878, "length_chars": 26}, {"completion": " (A) Jupiter\n\nThe planet is Venus.\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1837482930277474, "length_chars": 35}, {"completion": " (C) Earth\n\nI want to know which planet", "correct": false, "details": {"choice": "C"}, "duration_s": 1.1024704159935936, "length_chars": 39}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 34.666666666666664, "avg_duration_s": 1.2354593679871566, "avg_chars_per_sec": 29.138959105269613}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: 9 * 7 = ?\nOptions:\n(A) 56\n(B) 63\n(C) 72\nAnswer:", "answer": "B", "samples": [{"completion": " (B) Answer: (C)\n(D)", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1106807420146652, "length_chars": 20}, {"completion": " 63\n\nQuestion: 12 * 7 = ?\nOptions", "correct": false, "details": {"choice": "Q"}, "duration_s": 1.0677297209622338, "length_chars": 33}, {"completion": " (A) 56 + (A) 63 + (C", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1217481200001203, "length_chars": 21}, {"completion": " (A) (B) (C) (D)", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1001307920087129, "length_chars": 16}, {"completion": " (A) * 7 / 2 = 4\n(B", "correct": false, "details": {"choice": "A"}, "duration_s": 1.106442723015789, "length_chars": 19}, {"completion": " (C)\n\nA:\n\nYour question is", "correct": false, "details": {"choice": "C"}, "duration_s": 1.0833043629536405, "length_chars": 26}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 21.5, "avg_duration_s": 1.0983394101591937, "avg_chars_per_sec": 20.485470877111513}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: Who wrote '1984'?\nOptions:\n(A) George Orwell\n(B) Aldous Huxley\n(C) Ray Bradbury\nAnswer:", "answer": "A", "samples": [{"completion": "\n(A) George Orwell\n(B) Ald", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1467931030201726, "length_chars": 26}, {"completion": " (C)\n(A)\n\nA:\n", "correct": false, "details": {"choice": "C"}, "duration_s": 1.1425529699772596, "length_chars": 13}, {"completion": " (A)\n(B)\n\nA:\n", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1552864470286295, "length_chars": 13}, {"completion": " (B).\n\nAnswers:\n(A)", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1428491299739107, "length_chars": 19}, {"completion": " Aldous Huxley, George Orwell\n\nA", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1494318610057235, "length_chars": 32}, {"completion": " Aldous. Orwell's first novel was '1984'", "correct": true, "details": {"choice": "A"}, "duration_s": 1.157381023047492, "length_chars": 40}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 22.5, "avg_duration_s": 1.1490490890088647, "avg_chars_per_sec": 20.741788633148172}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: Water's chemical formula is?\nOptions:\n(A) H2O\n(B) CO2\n(C) O2\nAnswer:", "answer": "A", "samples": [{"completion": " B\n\nA:\n\nYes, it is H", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1417995120282285, "length_chars": 20}, {"completion": " B\n(D) H2O\nAnswer: C", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1514825519989245, "length_chars": 20}, {"completion": " (A) (C)\n\nA:\n\n", "correct": true, "details": {"choice": "A"}, "duration_s": 1.139067203970626, "length_chars": 14}, {"completion": " H2O\nQuestion: A water molecule has 4 electrons", "correct": false, "details": {"choice": "H"}, "duration_s": 1.137823051016312, "length_chars": 47}, {"completion": " (A) Water is a molecule with a nucleus, which", "correct": true, "details": {"choice": "A"}, "duration_s": 1.150271491962485, "length_chars": 46}, {"completion": " Water's chemical formula is H2O.\n\nThe", "correct": false, "details": {"choice": "W"}, "duration_s": 1.1439197350409813, "length_chars": 38}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 29.5, "avg_duration_s": 1.1440605910029262, "avg_chars_per_sec": 26.950787026326708}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: 15 divided by 5 equals?\nOptions:\n(A) 2\n(B) 3\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": "\n16\nThe reason why you should be in A would", "correct": false, "details": {"choice": "T"}, "duration_s": 1.14478616300039, "length_chars": 43}, {"completion": " 6.7\n\nA:\n\nIt seems like", "correct": false, "details": {"choice": "A"}, "duration_s": 1.100004288018681, "length_chars": 23}, {"completion": " 15 divided by 3 equals?\n\nA:\n\n", "correct": false, "details": {"choice": "D"}, "duration_s": 1.1444003109936602, "length_chars": 30}, {"completion": " 4\n\nI have come up with the following logic:", "correct": false, "details": {"choice": "I"}, "duration_s": 1.1400771670159884, "length_chars": 44}, {"completion": " B\n\nYou can convert it to this:\n\n", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1401707360055298, "length_chars": 33}, {"completion": " 5\n\n(A)\n2\n(B)", "correct": false, "details": {"choice": "A"}, "duration_s": 1.137295609980356, "length_chars": 13}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 29.333333333333332, "avg_duration_s": 1.1344557125024342, "avg_chars_per_sec": 27.325879413678287}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: Largest mammal?\nOptions:\n(A) Elephant\n(B) Blue Whale\n(C) Giraffe\nAnswer:", "answer": "B", "samples": [{"completion": " A\n(D) Tiger\n\nA:\n\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1485698479809798, "length_chars": 18}, {"completion": " The largest mammal is the elephant. The largest bird is", "correct": false, "details": {"choice": "T"}, "duration_s": 1.1556709599681199, "length_chars": 56}, {"completion": " (A)\nOptions:\n(A) Giraffe", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1226540179923177, "length_chars": 25}, {"completion": " 1.\nAnswer: 3.\nAnswer: 4.", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1461903079762124, "length_chars": 25}, {"completion": " (C)\n\n(A)\n\n(B", "correct": false, "details": {"choice": "C"}, "duration_s": 1.1517537760082632, "length_chars": 13}, {"completion": " (A)\n(B)\n(C)\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1480331019847654, "length_chars": 13}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 23.5, "avg_duration_s": 1.1454786686517764, "avg_chars_per_sec": 21.824937193658}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: Which language is primarily spoken in Brazil?\nOptions:\n(A) Spanish\n(B) Portuguese\n(C) French\nAnswer:", "answer": "B", "samples": [{"completion": " (A)\n\nA:\n\nThe languages spoken", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1218652110546827, "length_chars": 30}, {"completion": " Brazil\n\nPortuguese: \nPortuguese = a linguagem", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1163990159984678, "length_chars": 46}, {"completion": " (A) Spanish\n\nA:\n\nEnglish is", "correct": false, "details": {"choice": "A"}, "duration_s": 1.2246667899889871, "length_chars": 28}, {"completion": " (C)\n\nI have used the following to test", "correct": false, "details": {"choice": "C"}, "duration_s": 1.1227435349719599, "length_chars": 39}, {"completion": " (A) Portuguese\nQuestion: How many people are on", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1469742970075458, "length_chars": 48}, {"completion": " A. This is the most popular language.\n(D", "correct": false, "details": {"choice": "A"}, "duration_s": 1.052025587996468, "length_chars": 41}], "metrics": {"pass_at": {"1": 0.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 37.666666666666664, "avg_duration_s": 1.1307790728363518, "avg_chars_per_sec": 34.19471371156386}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
{"prompt": "Question: What is 12 - 5?\nOptions:\n(A) 6\n(B) 7\n(C) 8\nAnswer:", "answer": "B", "samples": [{"completion": " (B)\n1) Which one of these questions is", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1369888510089368, "length_chars": 39}, {"completion": " 6\n\nCan someone please help me with this code?", "correct": false, "details": {"choice": "C"}, "duration_s": 1.1338096829713322, "length_chars": 46}, {"completion": " (A) 6\n(B) 7\n(C", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1290162660297938, "length_chars": 15}, {"completion": " (B)\n\nSo this is how I would write", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1754416120238602, "length_chars": 34}, {"completion": " (A)\n(B)\n(C)\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.118882788985502, "length_chars": 13}, {"completion": " (B)\n1\n2\n3\nAnswer:", "correct": true, "details": {"choice": "B"}, "duration_s": 1.113411233003717, "length_chars": 18}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 26.333333333333332, "avg_duration_s": 1.134591739003857, "avg_chars_per_sec": 24.237793256052004}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251102T093808Z"}
