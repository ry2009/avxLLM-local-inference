{"prompt": "Question: 2 + 2 = ?\nOptions:\n(A) 3\n(B) 4\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": "  4 + 4 = 6\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 2.909577337966766, "length_chars": 15}, {"completion": "  4 + 4 = 6\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0637483239988796, "length_chars": 15}, {"completion": "  4 + 4 = 6\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0845686490065418, "length_chars": 15}, {"completion": "  4 + 4 = 6\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0698202750063501, "length_chars": 15}, {"completion": "  4 + 4 = 6\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0570683060213923, "length_chars": 15}, {"completion": "  4 + 4 = 6\n\nA:", "correct": false, "details": {"choice": "A"}, "duration_s": 1.074953842035029, "length_chars": 15}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 13, "avg_duration_s": 1.3766227890058265, "avg_chars_per_sec": 10.896231066196968}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: What is the capital of Germany?\nOptions:\n(A) Berlin\n(B) Munich\n(C) Hamburg\nAnswer:", "answer": "A", "samples": [{"completion": "  B\n(D)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 1.0611328320228495, "length_chars": 11}, {"completion": "  B\n(D) Germany\nAnswer:  D", "correct": false, "details": {"choice": "B"}, "duration_s": 1.16739952203352, "length_chars": 26}, {"completion": "  B\n(D)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 1.0413217630120926, "length_chars": 11}, {"completion": "  B\n(D) Germany\nAnswer:  D", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1696495820069686, "length_chars": 26}, {"completion": "  B\n(D)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 1.0568800999899395, "length_chars": 11}, {"completion": "  B\n(D)\n\nA:", "correct": false, "details": {"choice": "B"}, "duration_s": 1.0715383019996807, "length_chars": 11}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.3333333333333333, "entropy": 0.6365141682928128, "avg_completion_length": 14, "avg_duration_s": 1.0946536835108418, "avg_chars_per_sec": 14.616494916167275}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: Which planet is known as the Red Planet?\nOptions:\n(A) Venus\n(B) Mars\n(C) Jupiter\nAnswer:", "answer": "B", "samples": [{"completion": "  Jupiter\n\nA:\n\n", "correct": false, "details": {"choice": "J"}, "duration_s": 0.8803137369686738, "length_chars": 15}, {"completion": "  Jupiter\n\nA:\n\n", "correct": false, "details": {"choice": "J"}, "duration_s": 0.8863906799815595, "length_chars": 15}, {"completion": "  Jupiter\n\nA:\n\n", "correct": false, "details": {"choice": "J"}, "duration_s": 0.8821633119951002, "length_chars": 15}, {"completion": "  Jupiter\n\nA:\n\n", "correct": false, "details": {"choice": "J"}, "duration_s": 0.8920030409935862, "length_chars": 15}, {"completion": "  Jupiter\n\nA:\n\n", "correct": false, "details": {"choice": "J"}, "duration_s": 0.8613946179975756, "length_chars": 15}, {"completion": "  Jupiter\n\nA:\n\n", "correct": false, "details": {"choice": "J"}, "duration_s": 0.9084092989796773, "length_chars": 15}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 11, "avg_duration_s": 0.885112447819362, "avg_chars_per_sec": 16.94699926202063}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: 9 * 7 = ?\nOptions:\n(A) 56\n(B) 63\n(C) 72\nAnswer:", "answer": "B", "samples": [{"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.895718413987197, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8764143269509077, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8858961489750072, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8674847870133817, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8767843819805421, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8843582589761354, "length_chars": 9}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 7, "avg_duration_s": 0.8811093863138618, "avg_chars_per_sec": 10.214395783083953}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: Who wrote '1984'?\nOptions:\n(A) George Orwell\n(B) Aldous Huxley\n(C) Ray Bradbury\nAnswer:", "answer": "A", "samples": [{"completion": "  \n(A) George Orwell\n(B)", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1884206979884766, "length_chars": 24}, {"completion": "  \n(A) George Orwell\n(B)", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1606214250205085, "length_chars": 24}, {"completion": "  \n(A) George Orwell\n(B)", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1857592919841409, "length_chars": 24}, {"completion": "  \n(A) George Orwell\n(B)", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1792209180421196, "length_chars": 24}, {"completion": "  \n(A) George Orwell\n(B)", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1635941729764454, "length_chars": 24}, {"completion": "  \n(A) George Orwell\n(B)", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1818050119909458, "length_chars": 24}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 21, "avg_duration_s": 1.1765702530004394, "avg_chars_per_sec": 20.39827195936343}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: Water's chemical formula is?\nOptions:\n(A) H2O\n(B) CO2\n(C) O2\nAnswer:", "answer": "A", "samples": [{"completion": "  (A)\n\nA:\n\n", "correct": true, "details": {"choice": "A"}, "duration_s": 0.9814523980021477, "length_chars": 11}, {"completion": "  (B)\n\nA:\n\n", "correct": false, "details": {"choice": "B"}, "duration_s": 0.9861371890292503, "length_chars": 11}, {"completion": "  (A)\n\nA:\n\n", "correct": true, "details": {"choice": "A"}, "duration_s": 0.9804454669938423, "length_chars": 11}, {"completion": "  (A)\n\nA:\n\n", "correct": true, "details": {"choice": "A"}, "duration_s": 0.9982443969929591, "length_chars": 11}, {"completion": "  (A)\n\nA:\n\n", "correct": true, "details": {"choice": "A"}, "duration_s": 0.9748173319967464, "length_chars": 11}, {"completion": "  (A)\n\nA:\n\n", "correct": true, "details": {"choice": "A"}, "duration_s": 0.989984467974864, "length_chars": 11}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.3333333333333333, "entropy": 0.4505612088643047, "avg_completion_length": 7, "avg_duration_s": 0.9851802084983016, "avg_chars_per_sec": 11.165469936477072}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: 15 divided by 5 equals?\nOptions:\n(A) 2\n(B) 3\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": "  A\n\nA:\n\n", "correct": false, "details": {"choice": "A"}, "duration_s": 0.776110093051102, "length_chars": 9}, {"completion": "  A\n\nA:\n\n", "correct": false, "details": {"choice": "A"}, "duration_s": 0.7985663490253501, "length_chars": 9}, {"completion": "  A\n\nA:\n\n", "correct": false, "details": {"choice": "A"}, "duration_s": 0.7939469359698705, "length_chars": 9}, {"completion": "  A\n\nA:\n\n", "correct": false, "details": {"choice": "A"}, "duration_s": 0.7827163849724457, "length_chars": 9}, {"completion": "  A\n\nA:\n\n", "correct": false, "details": {"choice": "A"}, "duration_s": 0.7910745330154896, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8759241590159945, "length_chars": 9}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.3333333333333333, "entropy": 0.4505612088643047, "avg_completion_length": 5.333333333333333, "avg_duration_s": 0.8030564091750421, "avg_chars_per_sec": 11.207182829467055}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: Largest mammal?\nOptions:\n(A) Elephant\n(B) Blue Whale\n(C) Giraffe\nAnswer:", "answer": "B", "samples": [{"completion": "  B\n\nA:\n\n", "correct": true, "details": {"choice": "B"}, "duration_s": 0.806882992037572, "length_chars": 9}, {"completion": "  B\n\nA:\n\n", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8124248990206979, "length_chars": 9}, {"completion": "  B\n\nA:\n\n", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8010382059728727, "length_chars": 9}, {"completion": "  B\n\nA:\n\n", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8058107149554417, "length_chars": 9}, {"completion": "  B\n\nA:\n\n", "correct": true, "details": {"choice": "B"}, "duration_s": 0.7970334499841556, "length_chars": 9}, {"completion": "  B\n\nA:\n\n", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8055646460270509, "length_chars": 9}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 5, "avg_duration_s": 0.8047924846662985, "avg_chars_per_sec": 11.183007012959106}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: Which language is primarily spoken in Brazil?\nOptions:\n(A) Spanish\n(B) Portuguese\n(C) French\nAnswer:", "answer": "B", "samples": [{"completion": "  Brazil\n\nPortuguese:\n\nPortuguese (Brazil)", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1398605610011145, "length_chars": 42}, {"completion": "  Brazil\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.7231308650225401, "length_chars": 12}, {"completion": "  Brazil\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.7080786050064489, "length_chars": 12}, {"completion": "  Brazil\n\nPortuguese: \nPortuguese: \n", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1609953439910896, "length_chars": 36}, {"completion": "  Brazil\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.6999261929886416, "length_chars": 12}, {"completion": "  Brazil\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 1.0776526359841228, "length_chars": 12}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.5, "entropy": 0.8675632284784613, "avg_completion_length": 18.666666666666668, "avg_duration_s": 0.9182740339989929, "avg_chars_per_sec": 22.86899032584758}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
{"prompt": "Question: What is 12 - 5?\nOptions:\n(A) 6\n(B) 7\n(C) 8\nAnswer:", "answer": "B", "samples": [{"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 1.0674152380088344, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8682589459931478, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8853947849711403, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8782354849972762, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8897720249951817, "length_chars": 9}, {"completion": "  (B)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 0.8821485440130346, "length_chars": 9}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 7, "avg_duration_s": 0.9118708371631025, "avg_chars_per_sec": 9.869818874786768}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12, "dtype": "float32"}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251102T094134Z"}
