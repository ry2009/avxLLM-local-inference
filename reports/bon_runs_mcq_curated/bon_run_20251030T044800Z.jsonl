{"prompt": "Question: 2 + 2 = ?\nOptions:\n(A) 3\n(B) 4\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 1.7212278489605524, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.28736117703374475, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2849508869694546, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.28273818898014724, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.3292773430002853, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.3222335840109736, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.5379648381591929, "avg_chars_per_sec": 3.7177150961085044}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: What is the capital of Germany?\nOptions:\n(A) Berlin\n(B) Munich\n(C) Hamburg\nAnswer:", "answer": "A", "samples": [{"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.27905137900961563, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.2831535520381294, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.280658786999993, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.2542195610003546, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.253037310030777, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.25381645595189184, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.26732284083846025, "avg_chars_per_sec": 7.481590401055831}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: Which planet is known as the Red Planet?\nOptions:\n(A) Venus\n(B) Mars\n(C) Jupiter\nAnswer:", "answer": "B", "samples": [{"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2532050590380095, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2766849499894306, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26996517699444667, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.27913387096486986, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26682219200301915, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2650816210079938, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.26848214499962825, "avg_chars_per_sec": 7.4492849422175516}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: 9 * 7 = ?\nOptions:\n(A) 56\n(B) 63\n(C) 72\nAnswer:", "answer": "B", "samples": [{"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2510059099877253, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.25631296198116615, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.28050295799039304, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.24093105702195317, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.23966122302226722, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.25373121903976426, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.2536908881738782, "avg_chars_per_sec": 7.883609909667753}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: Who wrote '1984'?\nOptions:\n(A) George Orwell\n(B) Aldous Huxley\n(C) Ray Bradbury\nAnswer:", "answer": "A", "samples": [{"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.26129268103977665, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.27440551295876503, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.2611996870255098, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.28505787800531834, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.2902995939948596, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.2766842999844812, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.27482327550145175, "avg_chars_per_sec": 7.277403983890131}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: Water's chemical formula is?\nOptions:\n(A) H2O\n(B) CO2\n(C) O2\nAnswer:", "answer": "A", "samples": [{"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.2701040770043619, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.24879644997417927, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.28036975098075345, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.25811100500868633, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.276566959975753, "length_chars": 2}, {"completion": " A", "correct": true, "details": {"choice": "A"}, "duration_s": 0.258879781991709, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.2654713374892405, "avg_chars_per_sec": 7.533770006643596}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: 15 divided by 5 equals?\nOptions:\n(A) 2\n(B) 3\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.24748681101482362, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.25318845600122586, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26039671897888184, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2495329580269754, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.3527465070364997, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26176381000550464, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.27085254351065186, "avg_chars_per_sec": 7.384091631841536}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: Largest mammal?\nOptions:\n(A) Elephant\n(B) Blue Whale\n(C) Giraffe\nAnswer:", "answer": "B", "samples": [{"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.27039590798085555, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2887639019754715, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.252299526007846, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.27188260696129873, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.27736401598667726, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26346086495323107, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.27069447064423, "avg_chars_per_sec": 7.388403594798847}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: Which language is primarily spoken in Brazil?\nOptions:\n(A) Spanish\n(B) Portuguese\n(C) French\nAnswer:", "answer": "B", "samples": [{"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26024410902755335, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2514853829634376, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.27470605698181316, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.25266132300021127, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26574599102605134, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.27042845397954807, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.2625452194964358, "avg_chars_per_sec": 7.6177353517844235}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
{"prompt": "Question: What is 12 - 5?\nOptions:\n(A) 6\n(B) 7\n(C) 8\nAnswer:", "answer": "B", "samples": [{"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.2543076050351374, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.3464581819716841, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.26147187303286046, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.27013607299886644, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.260953460005112, "length_chars": 2}, {"completion": " B", "correct": true, "details": {"choice": "B"}, "duration_s": 0.25070285197580233, "length_chars": 2}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 0.16666666666666666, "entropy": -1.000088900581841e-12, "avg_completion_length": 1, "avg_duration_s": 0.2740050075032438, "avg_chars_per_sec": 7.299136677187635}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.1, "top_p": 0.8, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": "mcq-distill", "adapter_path": "adapters/mcq-distill/mcq-distill"}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-adapter"}, "timestamp": "20251030T044800Z"}
