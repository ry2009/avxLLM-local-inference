{"prompt": "Question: 2 + 2 = ?\nOptions:\n(A) 3\n(B) 4\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": "\n2 + 3 = ?\nOption B:\n3", "correct": false, "details": {"choice": "O"}, "duration_s": 4.159723532968201, "length_chars": 22}, {"completion": " 3 + 4 + 5 = ?\n\nMy question is", "correct": false, "details": {"choice": "M"}, "duration_s": 1.0872630730154924, "length_chars": 30}, {"completion": " 5\n\nA:\n\nAnswer: 5\n1", "correct": false, "details": {"choice": "A"}, "duration_s": 1.09597876801854, "length_chars": 19}, {"completion": " 4 + 2 = 6\n\nA:\n\nIn", "correct": false, "details": {"choice": "A"}, "duration_s": 1.079452155972831, "length_chars": 18}, {"completion": " 4 + 3 = ?\n(D) 3 + 5", "correct": false, "details": {"choice": "D"}, "duration_s": 1.0628547300002538, "length_chars": 20}, {"completion": " 5 + 5 = ?\n\nI want to find a", "correct": false, "details": {"choice": "I"}, "duration_s": 1.094462768000085, "length_chars": 28}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 21.833333333333332, "avg_duration_s": 1.596622504662567, "avg_chars_per_sec": 14.30102185498066}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: What is the capital of Germany?\nOptions:\n(A) Berlin\n(B) Munich\n(C) Hamburg\nAnswer:", "answer": "A", "samples": [{"completion": " Berlin\n\nA:\n\nAs far as I know", "correct": false, "details": {"choice": "B"}, "duration_s": 1.071853090019431, "length_chars": 29}, {"completion": " Germany is capital of German country. Germany is Capital of German", "correct": false, "details": {"choice": "G"}, "duration_s": 1.1653208089992404, "length_chars": 67}, {"completion": " (B)\n\nGerman: \n1. Das", "correct": false, "details": {"choice": "B"}, "duration_s": 1.0892991760047153, "length_chars": 21}, {"completion": "\n(D) Munich\n(E) Berlin\n(", "correct": false, "details": {"choice": "D"}, "duration_s": 1.2042654380202293, "length_chars": 24}, {"completion": " B\n\nA:\n\nA, B, and", "correct": false, "details": {"choice": "B"}, "duration_s": 1.0906831810134463, "length_chars": 17}, {"completion": "\nA. B\nB. C\nB. D", "correct": true, "details": {"choice": "A"}, "duration_s": 1.2091599110281095, "length_chars": 15}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 27.833333333333332, "avg_duration_s": 1.1384302675141953, "avg_chars_per_sec": 25.327272259101107}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: Which planet is known as the Red Planet?\nOptions:\n(A) Venus\n(B) Mars\n(C) Jupiter\nAnswer:", "answer": "B", "samples": [{"completion": " (B)\n\n(A)\n\n(B", "correct": true, "details": {"choice": "B"}, "duration_s": 1.0973041060497053, "length_chars": 13}, {"completion": " (C)\nThe first two planets are all in the", "correct": false, "details": {"choice": "C"}, "duration_s": 1.0823943709838204, "length_chars": 41}, {"completion": " It is known as the Red Planet, or, Mars (", "correct": false, "details": {"choice": "I"}, "duration_s": 1.0983361360267736, "length_chars": 42}, {"completion": " (B) Mars\n\nCan you use this as an", "correct": true, "details": {"choice": "B"}, "duration_s": 1.0727482149959542, "length_chars": 33}, {"completion": "\n(A) Venus\n(B) Mars\n(", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0866574289975688, "length_chars": 21}, {"completion": " (A)\n(B)\n(C)\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0984128970303573, "length_chars": 13}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 26, "avg_duration_s": 1.0893088590140299, "avg_chars_per_sec": 24.939360808335053}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: 9 * 7 = ?\nOptions:\n(A) 56\n(B) 63\n(C) 72\nAnswer:", "answer": "B", "samples": [{"completion": " (A)\nAnswer: (B)\nAnswer:", "correct": false, "details": {"choice": "A"}, "duration_s": 1.2273300979868509, "length_chars": 24}, {"completion": " (A)\n(B)\n\nA:\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.2134565710439347, "length_chars": 13}, {"completion": " ?\n\nI can get the answer from the given number", "correct": false, "details": {"choice": "I"}, "duration_s": 1.169123578001745, "length_chars": 46}, {"completion": " (A)\n\n*7 * 5 = 6\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.515259995998349, "length_chars": 17}, {"completion": " A\n(B)\n\nHow do I check that", "correct": false, "details": {"choice": "A"}, "duration_s": 1.291746482020244, "length_chars": 27}, {"completion": " (A)\nA is a square (if your answer", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1946405390044674, "length_chars": 34}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 0.0, "16": 0.0, "32": 0.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 25.5, "avg_duration_s": 1.2685928773425985, "avg_chars_per_sec": 21.152044767540243}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: Who wrote '1984'?\nOptions:\n(A) George Orwell\n(B) Aldous Huxley\n(C) Ray Bradbury\nAnswer:", "answer": "A", "samples": [{"completion": " Huxley, I think, because he is the most", "correct": false, "details": {"choice": "H"}, "duration_s": 1.148189186991658, "length_chars": 40}, {"completion": "\n(A) George Orwell\n(B) Ald", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1102036989759654, "length_chars": 26}, {"completion": " Aldous Huxley (the writer)\n\nA", "correct": true, "details": {"choice": "A"}, "duration_s": 1.268839341995772, "length_chars": 30}, {"completion": " (B)\n\nThe following people wrote or co-", "correct": false, "details": {"choice": "B"}, "duration_s": 1.1704636839567684, "length_chars": 39}, {"completion": " (A)\n\n(A) This could be a", "correct": true, "details": {"choice": "A"}, "duration_s": 1.866200840973761, "length_chars": 25}, {"completion": " Aldous Huxley\n\nA:\n\nAnswer", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1520172670134343, "length_chars": 26}], "metrics": {"pass_at": {"1": 0.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 30, "avg_duration_s": 1.2859856699845598, "avg_chars_per_sec": 24.106022892441874}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: Water's chemical formula is?\nOptions:\n(A) H2O\n(B) CO2\n(C) O2\nAnswer:", "answer": "A", "samples": [{"completion": " (A) H2O\n\nI am getting this", "correct": true, "details": {"choice": "A"}, "duration_s": 1.149010499008, "length_chars": 27}, {"completion": " (A)\n\nA:\n\nOxygen is", "correct": true, "details": {"choice": "A"}, "duration_s": 1.0975703719886951, "length_chars": 19}, {"completion": " (B)\n\nA:\n\nWater is H", "correct": false, "details": {"choice": "B"}, "duration_s": 1.0833438959671184, "length_chars": 20}, {"completion": " H2O\nOption #1:\nSolution:\n", "correct": false, "details": {"choice": "H"}, "duration_s": 1.0798698069993407, "length_chars": 26}, {"completion": " (C)\n(A)\nThe reason I'm", "correct": false, "details": {"choice": "C"}, "duration_s": 1.1110014609876089, "length_chars": 23}, {"completion": " A\n\nA:\n\nThe answer would be:", "correct": true, "details": {"choice": "A"}, "duration_s": 1.1018254810478538, "length_chars": 28}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 22.666666666666668, "avg_duration_s": 1.1037702526664361, "avg_chars_per_sec": 21.59265777978514}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: 15 divided by 5 equals?\nOptions:\n(A) 2\n(B) 3\n(C) 5\nAnswer:", "answer": "B", "samples": [{"completion": " 6.9\n\nA:\n\nAs @c", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0893099390086718, "length_chars": 15}, {"completion": " 2 + 3 + 5\n(A) 2 + 3", "correct": false, "details": {"choice": "A"}, "duration_s": 1.100144432974048, "length_chars": 20}, {"completion": " 20\n\nA:\n\nI'd try to go", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1054452029638924, "length_chars": 22}, {"completion": " 5 divided by 5 equals\nOptions:\n(A)", "correct": false, "details": {"choice": "D"}, "duration_s": 1.100038937991485, "length_chars": 35}, {"completion": " 2 (B) (A)\n(C) 3", "correct": true, "details": {"choice": "B"}, "duration_s": 1.0927391749573871, "length_chars": 16}, {"completion": " 5.\n(D) 6\n(E) 7", "correct": false, "details": {"choice": "D"}, "duration_s": 1.0969710919889621, "length_chars": 15}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 19.5, "avg_duration_s": 1.0974414633140743, "avg_chars_per_sec": 18.679811803441176}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: Largest mammal?\nOptions:\n(A) Elephant\n(B) Blue Whale\n(C) Giraffe\nAnswer:", "answer": "B", "samples": [{"completion": " Elephant.\n(A)\n(B)\n", "correct": false, "details": {"choice": "E"}, "duration_s": 1.1013664389611222, "length_chars": 19}, {"completion": " elephant\n\nA:\n\nThere is no limit to", "correct": false, "details": {"choice": "E"}, "duration_s": 1.1287491290131584, "length_chars": 35}, {"completion": " (C)\n\nA. Elephant\nB.", "correct": false, "details": {"choice": "C"}, "duration_s": 1.109765266010072, "length_chars": 20}, {"completion": " elephant\n\nQuestion: What animal is the largest mammal", "correct": false, "details": {"choice": "E"}, "duration_s": 1.1193751709652133, "length_chars": 54}, {"completion": " (A)  \n\nIn this case, I can", "correct": false, "details": {"choice": "A"}, "duration_s": 1.1031832129810937, "length_chars": 27}, {"completion": "\n(B)\n(C)\n\nA:", "correct": true, "details": {"choice": "B"}, "duration_s": 1.1155939889722504, "length_chars": 12}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 0.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 26.666666666666668, "avg_duration_s": 1.1130055344838183, "avg_chars_per_sec": 25.00736292047431}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: Which language is primarily spoken in Brazil?\nOptions:\n(A) Spanish\n(B) Portuguese\n(C) French\nAnswer:", "answer": "B", "samples": [{"completion": " (B)\n\nA:\n\nFirst of all", "correct": true, "details": {"choice": "B"}, "duration_s": 1.0687762019806542, "length_chars": 22}, {"completion": " (A)\n(B)\n(C)\n", "correct": false, "details": {"choice": "A"}, "duration_s": 1.069585420016665, "length_chars": 13}, {"completion": "\n* (A) (B)\n* (C", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0651914390036836, "length_chars": 15}, {"completion": " Portuguese and Spanish\n\nWhich language is mainly spoken in Peru", "correct": false, "details": {"choice": "P"}, "duration_s": 1.069667763018515, "length_chars": 64}, {"completion": " A\n\nA:\n\nThe languages spoken in Brazil", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0714335690136068, "length_chars": 38}, {"completion": " (C)\n\nA:\n\nBrazilian.", "correct": false, "details": {"choice": "C"}, "duration_s": 1.0744552100077271, "length_chars": 20}], "metrics": {"pass_at": {"1": 1.0, "2": 1.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 27.5, "avg_duration_s": 1.0698516005068086, "avg_chars_per_sec": 26.794993486093524}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
{"prompt": "Question: What is 12 - 5?\nOptions:\n(A) 6\n(B) 7\n(C) 8\nAnswer:", "answer": "B", "samples": [{"completion": " 6 - 7\n\nWhat is 12?\nWhat is", "correct": false, "details": {"choice": "W"}, "duration_s": 1.0682254870189354, "length_chars": 27}, {"completion": " 6\n\nA:\n\nTry this:\n(", "correct": false, "details": {"choice": "A"}, "duration_s": 1.0581455350038595, "length_chars": 19}, {"completion": " 8\n\nMy SQLFiddle: http://www.", "correct": false, "details": {"choice": "M"}, "duration_s": 1.0665899959858507, "length_chars": 29}, {"completion": " (B) 7\n\nIn the previous example, 2", "correct": true, "details": {"choice": "B"}, "duration_s": 1.056719245039858, "length_chars": 34}, {"completion": " (B)\n\nWhat is 12 + 0?\n", "correct": true, "details": {"choice": "B"}, "duration_s": 1.0867284630076028, "length_chars": 22}, {"completion": " (C) Answer:\n\nA:\n\nAnswer", "correct": false, "details": {"choice": "C"}, "duration_s": 1.056473331991583, "length_chars": 24}], "metrics": {"pass_at": {"1": 0.0, "2": 0.0, "4": 1.0, "8": 1.0, "16": 1.0, "32": 1.0}, "unique_frac": 1.0, "entropy": 1.791759469222055, "avg_completion_length": 24.666666666666668, "avg_duration_s": 1.0654803430079482, "avg_chars_per_sec": 24.245715561869}, "config": {"k_values": [1, 2, 4, 8, 16, 32], "samples_per_prompt": 6, "temperature": 0.9, "top_p": 0.95, "max_new_tokens": 12}, "adapter": {"base_model": "EleutherAI/pythia-410m", "adapter_name": null, "adapter_path": null}, "scorer": "mc", "metadata": {"experiment": "bon-mcq-base"}, "timestamp": "20251030T043910Z"}
